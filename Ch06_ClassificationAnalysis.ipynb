{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Ch06_ClassificationAnalysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roitraining/SparkforDataEngineers/blob/Development/Ch06_ClassificationAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVESJgGmrOnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/home/student/ROI/SparkProgram')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy\n",
        "\n",
        "import matplotlib as mp\n",
        "from matplotlib import pyplot as plt\n",
        "#from IPython.display import display\n",
        "\n",
        "import pyspark_helpers as pyh\n",
        "sc, spark, conf = pyh.initspark()\n",
        "from pyspark_helpers import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOtG6pXJrOnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'bank.csv'\n",
        "df = spark.read.csv(f'/home/student/ROI/Spark/datasets/finance/{filename}', header = True, inferSchema = True)\n",
        "display(df)\n",
        "\n",
        "# Save a pointer to the raw data\n",
        "dfRawFile = df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-FZqXtsrOnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's just keep a few fields to start with for simplicity\n",
        "numeric_features = ['age','balance', 'duration', 'pdays']\n",
        "categorical_features = ['job', 'marital', 'education', 'housing', 'loan', 'contact', 'campaign', 'poutcome', 'deposit']\n",
        "\n",
        "# numeric_features = ['balance', 'duration', 'age']\n",
        "# categorical_features = ['marital', 'education']\n",
        "target_label = 'default'\n",
        "\n",
        "\n",
        "df = dfRawFile.select(numeric_features + categorical_features + [target_label])\n",
        "display(df)\n",
        "print(df.take(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27HGsZ6ArOnW",
        "colab_type": "text"
      },
      "source": [
        "### Explore numeric features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMFO0oVOrOnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "# import imp\n",
        "# imp.reload(pyh)\n",
        "\n",
        "pyh.describe_numeric_features(df, numeric_features)\n",
        "pyh.scatter_matrix(df, numeric_features)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtWabts7rOnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "# print('Convert string features to indexed numeric values')\n",
        "# df1 = pyh.StringIndexEncode(df, categorical_features + [target_label])\n",
        "# display(df1)\n",
        "\n",
        "# print('Convert indexed features to OneHotEncoded sparse matrix')\n",
        "# df2 = pyh.OneHotEncode(df1, categorical_features)\n",
        "# display(df2)\n",
        "\n",
        "# print('Assemble all features and target label into vectors')\n",
        "# df3 = pyh.AssembleFeatures(df2, categorical_features, numeric_features, target_label + '_Index')\n",
        "# display(df3)\n",
        "# df3.printSchema()\n",
        "\n",
        "dfML = pyh.MakeMLDataFrame(df, categorical_features, numeric_features, target_label)\n",
        "display(dfML)\n",
        "dfML.printSchema()\n",
        "labelCnt = dfML.groupBy('label').count()\n",
        "display(labelCnt)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw_RBw9xrOnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labelCnt.toPandas().plot(kind = 'bar')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO5xBgHErOnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AevDiyFrOnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfML.write.format('libsvm').save('testsave')\n",
        "# dfML = spark.read.format('libsvm').load('testsave')\n",
        "# x.printSchema()\n",
        "# display(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ5WvSOErOnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = dfML.randomSplit([.7,.3], seed = 1000)\n",
        "print (f'Training set row count {train.count()}')\n",
        "print (f'Testing set row count {test.count()}')\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnOwIT8_rOnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
        "dtModel = dt.fit(train)\n",
        "print('DT Trained')\n",
        "\n",
        "filename1 = filename.replace('.','_') + '_DT_trainedModel'\n",
        "dtModel.write().overwrite().save(filename1)\n",
        "print('DT Saved')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EByE2E-WrOn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtPredictions, dtLog = pyh.predict_and_evaluate(dtModel, test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goWOCubRrOn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
        "lrModel = lr.fit(train)\n",
        "print('LR Trained')\n",
        "\n",
        "filename1 = filename.replace('.','_') + '_LR_trainedModel'\n",
        "lrModel.write().overwrite().save(filename1)\n",
        "print('LR Saved')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVPv5t1irOn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lrModel2 = LogisticRegression.load(filename1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGB_IDUUrOoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrPredictions, lrLog = pyh.predict_and_evaluate(lrModel, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoN-fiT2rOoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
        "rfModel = rf.fit(train)\n",
        "print('RF Trained')\n",
        "\n",
        "filename1 = filename.replace('.','_') + '_RF_trainedModel'\n",
        "rfModel.write().overwrite().save(filename1)\n",
        "print('RF Saved')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF0GSzKXrOoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rfPredictions, rfLog = pyh.predict_and_evaluate(rfModel, test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90o6tiojrOoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbt = GBTClassifier(maxIter=10)\n",
        "gbtModel = gbt.fit(train)\n",
        "print ('GBT Trained')\n",
        "\n",
        "filename1 = filename.replace('.','_') + '_GBT_trainedModel'\n",
        "rfModel.write().overwrite().save(filename1)\n",
        "print ('GBT Saved')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wuna45UTrOoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gbtPredictions, gbtLog = pyh.predict_and_evaluate(gbtModel, test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QPLoCu1rOoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# specify layers for the neural network:\n",
        "# input layer of size 13 (features), two intermediate of size 5 and 4\n",
        "# and output of size 2 (classes)\n",
        "layers = [13, 5, 4, 2]\n",
        "\n",
        "nn = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
        "nnModel = nn.fit(train)\n",
        "print ('NN Trained')\n",
        "\n",
        "filename1 = filename.replace('.','_') + '_NN_trainedModel'\n",
        "nnModel.write().overwrite().save(filename1)\n",
        "print ('NN Saved')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blt7nlLJrOoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nnPredictions = nnModel.transform(test)\n",
        "print(nnPredictions)\n",
        "nnPredictions.printSchema()\n",
        "print (nnPredictions.count())\n",
        "nnPredictions.take(1)\n",
        "# predictionAndLabels = nnPredictions.select(\"prediction\", \"label\")\n",
        "# #display(predictionAndLabels)\n",
        "# print(predictionAndLabels.collect())\n",
        "# evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "# print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiZnj3adrOot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}