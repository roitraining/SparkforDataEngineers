{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/roitraining/SparkProgram/blob/Day2/Day2/Ch03_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JdIfR70Jxu2"
   },
   "source": [
    "Set up the Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "os_Mz8CUJxu5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/student/ROI/SparkProgram')\n",
    "from initspark import *\n",
    "sc, spark, conf = initspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vl7-cmUJxu-"
   },
   "source": [
    "Turn a simple RDD into a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sou-X5kCJxu_",
    "outputId": "7567ff27-c76b-412e-8378-1f551774057e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(1,'alpha'),(2,'beta')])\n",
    "x0 = spark.createDataFrame(x)\n",
    "x0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_1Cg_xuJxvE"
   },
   "source": [
    "Give the DataFrame meaningful column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udSoOBO5JxvF",
    "outputId": "e47d27a6-51d3-4902-aca9-f5393a4c8eba"
   },
   "outputs": [],
   "source": [
    "x1 = spark.createDataFrame(x, schema=['ID','Name'])\n",
    "x1.show()\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiGdbXBVJxvJ"
   },
   "source": [
    "Give a DataFrame a schema with column names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "en8OdGGlJxvK",
    "outputId": "6b25d060-f5f2-4451-b986-8f07e789247e"
   },
   "outputs": [],
   "source": [
    "x2 = spark.createDataFrame(x, 'ID:int, Name:string')\n",
    "x2.show()\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlhoOJwZJxvO"
   },
   "source": [
    "Load a text file into a RDD and clean it up as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxFb9sk4JxvQ",
    "outputId": "939268e2-cd0b-41b5-a4f1-b592a2f6e496"
   },
   "outputs": [],
   "source": [
    "filename = '/home/student/ROI/Spark/datasets/finance/CreditCard.csv'\n",
    "cc = sc.textFile(filename)\n",
    "first = cc.first()\n",
    "cc = cc.filter(lambda x : x != first)\n",
    "cc.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6aNyMRpJxvV",
    "outputId": "8ecbe5f7-9064-4549-de3c-76ac66da8fe8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "cc = cc.map(lambda x : x.split(',')) \n",
    "cc.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8w-d27x5JxvZ",
    "outputId": "205ca7ff-ba4f-453a-a00b-731542e21862"
   },
   "outputs": [],
   "source": [
    "cc = cc.map(lambda x : (x[0][1:], x[1][1:-1], datetime.datetime.strptime(x[2], '%d-%b-%y').date(), x[3], x[4], x[5], float(x[6])))\n",
    "print (cc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mk7f9YAWJxvc"
   },
   "source": [
    "Turn the RDD into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-PLhNetuJxvd",
    "outputId": "31ddfc38-009f-424d-90f3-9740594a268f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(cc)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XT2o_wyuJxvi"
   },
   "source": [
    "The built in toDF method does the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdjwqSI6Jxvj",
    "outputId": "fc9f2103-29a5-48f1-c212-96222387a9a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = cc.toDF()\n",
    "df.show()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3X8dukbpJxvo",
    "outputId": "8417853a-85e0-4640-ba1d-6860e763e035",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = cc.toDF(['City', 'Country', 'Date', 'CardType', 'TranType', 'Gender', 'Amount'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2iXnrkJJxvu",
    "outputId": "f3eb75b8-e268-4da1-c3e7-a32b2187b0e4"
   },
   "outputs": [],
   "source": [
    "df = cc.toDF('City: string, Country: string, Date: date, CardType: string, TranType: string, Gender: string, Amount: double')\n",
    "df.show()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdAF7yVmJxvy"
   },
   "source": [
    "**LAB:** Use the regions and territories RDDs from the previous lab and convert them into DataFrames with meaningful schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIbiLKz0Jxvz"
   },
   "outputs": [],
   "source": [
    "regions = sc.textFile('hdfs://localhost:9000/regions')\n",
    "regions = regions.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1]))\n",
    "\n",
    "territories = sc.textFile('hdfs://localhost:9000/territories')\n",
    "territories = territories.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[2])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqpW0HPKJxv2"
   },
   "source": [
    "Convert a DataFrame into a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I64RwvUdJxv3",
    "outputId": "2be2fd86-cd1f-451d-a79e-f70a750a9364"
   },
   "outputs": [],
   "source": [
    "print (df.toJSON().take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0JDyefiJxv7",
    "outputId": "1d942923-c7cb-474b-f04e-eb12e4269842"
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "print (df.columns, df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2PVAJGSJxv-"
   },
   "source": [
    "Choose particular columns from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shp65QEQJxv_",
    "outputId": "6ef0da8a-dbc5-4641-92b3-8ec6ac511261"
   },
   "outputs": [],
   "source": [
    "df.select('City', 'Country', 'Amount').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOXLR2IPJxwD",
    "outputId": "705a7a1a-505a-4869-c56e-7ebe7ea28c8b"
   },
   "outputs": [],
   "source": [
    "df.select('City', 'Country').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9NySqdqJxwH"
   },
   "source": [
    "Sort a DataFrame. The sort and orderBy methods are different aliases for the exact same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i9IkEP-4JxwI",
    "outputId": "a5d850fa-3822-4771-9b2c-c605d6283315"
   },
   "outputs": [],
   "source": [
    "df.sort(df.Amount).show()\n",
    "df.sort(df.Amount, ascending = False).show()\n",
    "df.select('City', 'Amount').orderBy(df.City).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6zM9-5MJxwS"
   },
   "source": [
    "Create a new DataFrame with a new calculated column added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfXZYr82JxwU",
    "outputId": "30b2412f-0689-4a4b-fa6b-8e7b2a6326ce"
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn('Discount', df.Amount * .03)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xImpvfmVJxwb"
   },
   "source": [
    "Remove an unwanted column from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvbveAjfJxwd",
    "outputId": "9fa2d404-1c17-4ab7-9e18-d31d0f15a017",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.drop(df2.Country)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsHhY9u9Jxwh"
   },
   "source": [
    "The filter and where methods can both be used and have alternative ways to represent the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bg7uhoEJxwj",
    "outputId": "749bae40-a8c1-4055-e1a8-098b78277474",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3.filter(df3.Amount < 4000).show()\n",
    "print(df3.filter('Amount < 4000').count())\n",
    "print(df3.where('Amount < 4000').count())\n",
    "print(df3.where(df3.Amount < 4000).count())\n",
    "\n",
    "print (df3.where((df3.Amount > 3000) & (df3.Amount < 4000)).count())\n",
    "print (df3.where('Amount > 3000 and Amount < 4000').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkrAyBg4Jxwo"
   },
   "source": [
    "Load a CSV file directly into a DataFrame using alternate syntaxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5E4afFTJxwp"
   },
   "source": [
    "**LAB:** Using the df3 DataFrame, answer the following questions:\n",
    "\n",
    "How many Platinum card purchases were there with a discount above $100?\n",
    "\n",
    "Find the ten biggest discount amounts earned by women and show just the purchase amount, discount, and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kZwJv9LJxwr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRYHW5DlJxwx"
   },
   "source": [
    "JOINs work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLGYJT1PJxwy",
    "outputId": "4d2e96d3-41a3-4eb2-fb88-b44e5a40dc6b"
   },
   "outputs": [],
   "source": [
    "tab1 = sc.parallelize([(1, 'Alpha'), (2, 'Beta'), (3, 'Delta')]).toDF('ID:int, code:string')\n",
    "tab2 = sc.parallelize([(100, 'One', 1), (101, 'Two', 2), (102, 'Three', 1), (103, 'Four', 4)]).toDF('ID:int, name:string, parentID:int')\n",
    "tab1.join(tab2, tab1.ID == tab2.parentID).show()\n",
    "tab1.join(tab2, tab1.ID == tab2.parentID, 'left').show()\n",
    "tab1.join(tab2, tab1.ID == tab2.parentID, 'right').show()\n",
    "tab1.join(tab2, tab1.ID == tab2.parentID, 'full').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbKSw8mFJxw1"
   },
   "source": [
    "Examples of aggregate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Q-LPpNaJxw2",
    "outputId": "d3074c8c-76a8-48be-f061-c9094006e358",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tab3 = sc.parallelize([(1, 10), (1, 20), (1, 30), (2, 40), (2,50)]).toDF('groupID:int, amount:int')\n",
    "tab3.groupby('groupID').max().show()\n",
    "tab3.groupby('groupID').sum().show()\n",
    "x = tab3.groupby('groupID')\n",
    "x.agg({'amount':'sum', 'amount':'max'}).show()\n",
    "from pyspark.sql import functions as F\n",
    "x.agg(F.sum('amount'), F.max('amount')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5J3KIQfJxw5"
   },
   "source": [
    "Examples of reading a CSV directly into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJWVKyXeJxw6"
   },
   "outputs": [],
   "source": [
    "filename = '/home/student/ROI/Spark/datasets/finance/CreditCard.csv'\n",
    "df4 = spark.read.load(filename, format = 'csv', sep = ',', inferSchema = True, header = True)\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GmIrAg1KJxxA"
   },
   "outputs": [],
   "source": [
    "df4 = spark.read.format('csv').option('header','true').option('inferSchema','true').load(filename)\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQVzZUyVJxxE"
   },
   "outputs": [],
   "source": [
    "df4 = spark.read.csv(filename, header = True, inferSchema = True)\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlpJkJKkJxxI"
   },
   "outputs": [],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDGFU2bwJxxN"
   },
   "source": [
    "**LAB:** Read the Products file from the JSON folder and categories from ths CSVHeaders folder, then join them displaying just the product and category IDs and names, and sort by categoryID then productID. \n",
    "\n",
    "Hint: Drop the ambiguous column after the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTxpSZd7JxxP",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pxxBU4AJxxS"
   },
   "source": [
    "Change the name of the City column to CityCountry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RUqQyieJxxU"
   },
   "outputs": [],
   "source": [
    "cols = df4.columns\n",
    "cols[0] = 'CityCountry'\n",
    "df4 = df4.toDF(*cols)\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pk-WlDwNJxxW"
   },
   "source": [
    "Apply a custom UDF to columns to separate the City and Country and convert the Date into a date datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4dcWXI1iJxxX"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_date\n",
    "import datetime\n",
    "\n",
    "def city(x):\n",
    "    return x[:x.find(',')]\n",
    "def country(x):\n",
    "    return x[x.find(',') + 1 :]\n",
    "\n",
    "df5 = df4.withColumn('City', udf(city, StringType())(df4.CityCountry)) \\\n",
    "      .withColumn('Country', udf(country, StringType())(df4.CityCountry)) \\\n",
    "      .withColumn('Date', to_date(df4.Date, 'dd-MMM-yy')) \\\n",
    "      .drop(df4.CityCountry)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJShtBBYJxxe"
   },
   "source": [
    "DataFrames can be written to a variety of file formats. Here we are writing it to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lds2CsoKJxxf"
   },
   "outputs": [],
   "source": [
    "df5.write.json('/home/student/Spark/CreditCard.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1hZdPH5Jxxm"
   },
   "source": [
    "Read a JSON file into a DataFrame, but note that we lose the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYzveIUNJxxn"
   },
   "outputs": [],
   "source": [
    "df6 = spark.read.json('/home/student/Spark/CreditCard.json')\n",
    "df6.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex6IIfYYJxxp"
   },
   "source": [
    "Create a schema that can be used to import a file and directly name the columns and convert them to the desired data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEYmzqk8Jxxq"
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('Date', DateType()), \n",
    "    StructField('Card Type', StringType()),\n",
    "    StructField('Exp Type', StringType()),\n",
    "    StructField('Gender', StringType()),\n",
    "    StructField('Amount', FloatType()),\n",
    "    StructField('City', StringType()),\n",
    "    StructField('Country', StringType())\n",
    "])\n",
    "df6 = spark.read.json('/home/student/Spark/CreditCard.json', schema = schema)\n",
    "df6.printSchema()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Ch03_DataFrames.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
