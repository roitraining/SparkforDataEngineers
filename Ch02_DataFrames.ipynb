{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch03_DataFrames.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roitraining/SparkforDataEngineers/blob/Development/Ch02_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-JdIfR70Jxu2"
      },
      "source": [
        "###Set up the Spark environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "os_Mz8CUJxu5",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/home/student/ROI/SparkProgram')\n",
        "from initspark import *\n",
        "sc, spark, conf = initspark()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6vl7-cmUJxu-"
      },
      "source": [
        "###Turn a simple RDD into a DataFrame. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sou-X5kCJxu_",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "x = sc.parallelize([(1,'alpha'),(2,'beta')])\n",
        "x0 = spark.createDataFrame(x)\n",
        "x0.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E_1Cg_xuJxvE"
      },
      "source": [
        "###Give the DataFrame meaningful column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "udSoOBO5JxvF",
        "colab": {}
      },
      "source": [
        "x1 = spark.createDataFrame(x, schema=['ID','Name'])\n",
        "x1.show()\n",
        "print(x1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uiGdbXBVJxvJ"
      },
      "source": [
        "###Give a DataFrame a schema with column names and data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "en8OdGGlJxvK",
        "colab": {}
      },
      "source": [
        "x2 = spark.createDataFrame(x, 'ID:int, Name:string')\n",
        "x2.show()\n",
        "print(x2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VlhoOJwZJxvO"
      },
      "source": [
        "###Load a text file into a RDD and clean it up as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YxFb9sk4JxvQ",
        "colab": {}
      },
      "source": [
        "filename = '/home/student/ROI/Spark/datasets/finance/CreditCard.csv'\n",
        "cc = sc.textFile(filename)\n",
        "first = cc.first()\n",
        "cc = cc.filter(lambda x : x != first)\n",
        "cc.take(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W6aNyMRpJxvV",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "cc = cc.map(lambda x : x.split(',')) \n",
        "cc.take(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8w-d27x5JxvZ",
        "colab": {}
      },
      "source": [
        "cc = cc.map(lambda x : (x[0][1:], x[1][1:-1], datetime.datetime.strptime(x[2], '%d-%b-%y').date(), x[3], x[4], x[5], float(x[6])))\n",
        "print (cc.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mk7f9YAWJxvc"
      },
      "source": [
        "###Turn the RDD into a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-PLhNetuJxvd",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "df = spark.createDataFrame(cc)\n",
        "df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XT2o_wyuJxvi"
      },
      "source": [
        "###The built in toDF method does the same thing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sdjwqSI6Jxvj",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "df = cc.toDF()\n",
        "df.show()\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3X8dukbpJxvo",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "df = cc.toDF(['City', 'Country', 'Date', 'CardType', 'TranType', 'Gender', 'Amount'])\n",
        "df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y2iXnrkJJxvu",
        "colab": {}
      },
      "source": [
        "df = cc.toDF('City: string, Country: string, Date: date, CardType: string, TranType: string, Gender: string, Amount: double')\n",
        "df.show()\n",
        "print(df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rdAF7yVmJxvy"
      },
      "source": [
        "###***LAB:*** Use the regions and territories RDDs from the previous lab and convert them into DataFrames with meaningful schemas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aIbiLKz0Jxvz",
        "colab": {}
      },
      "source": [
        "regions = sc.textFile('hdfs://localhost:9000/regions')\n",
        "regions = regions.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1]))\n",
        "\n",
        "territories = sc.textFile('hdfs://localhost:9000/territories')\n",
        "territories = territories.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[2])))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CqpW0HPKJxv2"
      },
      "source": [
        "###Convert a DataFrame into a JSON string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I64RwvUdJxv3",
        "colab": {}
      },
      "source": [
        "print (df.toJSON().take(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v0JDyefiJxv7",
        "colab": {}
      },
      "source": [
        "df.printSchema()\n",
        "print (df.columns, df.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s2PVAJGSJxv-"
      },
      "source": [
        "###Choose particular columns from a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "shp65QEQJxv_",
        "colab": {}
      },
      "source": [
        "df.select('City', 'Country', 'Amount').show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xOXLR2IPJxwD",
        "colab": {}
      },
      "source": [
        "df.select('City', 'Country').distinct().show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w9NySqdqJxwH"
      },
      "source": [
        "###Sort a DataFrame. The sort and orderBy methods are different aliases for the exact same method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i9IkEP-4JxwI",
        "colab": {}
      },
      "source": [
        "df.sort(df.Amount).show()\n",
        "df.sort(df.Amount, ascending = False).show()\n",
        "df.select('City', 'Amount').orderBy(df.City).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6zM9-5MJxwS"
      },
      "source": [
        "###Create a new DataFrame with a new calculated column added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XfXZYr82JxwU",
        "colab": {}
      },
      "source": [
        "df2 = df.withColumn('Discount', df.Amount * .03)\n",
        "df2.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xImpvfmVJxwb"
      },
      "source": [
        "###Remove an unwanted column from a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AvbveAjfJxwd",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "df3 = df2.drop(df2.Country)\n",
        "df3.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZsHhY9u9Jxwh"
      },
      "source": [
        "###The filter and where methods can both be used and have alternative ways to represent the condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-bg7uhoEJxwj",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "df3.filter(df3.Amount < 4000).show()\n",
        "print(df3.filter('Amount < 4000').count())\n",
        "print(df3.where('Amount < 4000').count())\n",
        "print(df3.where(df3.Amount < 4000).count())\n",
        "\n",
        "print (df3.where((df3.Amount > 3000) & (df3.Amount < 4000)).count())\n",
        "print (df3.where('Amount > 3000 and Amount < 4000').count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IkrAyBg4Jxwo"
      },
      "source": [
        "###Load a CSV file directly into a DataFrame using alternate syntaxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k5E4afFTJxwp"
      },
      "source": [
        "###***LAB:*** Using the df3 DataFrame, answer the following questions:\n",
        "\n",
        "\n",
        "*   How many Platinum card purchases were there with a discount above $100?\n",
        "*   Find the ten biggest discount amounts earned by women and show just the purchase amount, discount, and date.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6kZwJv9LJxwr",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yRYHW5DlJxwx"
      },
      "source": [
        "###JOINs work as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wLGYJT1PJxwy",
        "colab": {}
      },
      "source": [
        "tab1 = sc.parallelize([(1, 'Alpha'), (2, 'Beta'), (3, 'Delta')]).toDF('ID:int, code:string')\n",
        "tab2 = sc.parallelize([(100, 'One', 1), (101, 'Two', 2), (102, 'Three', 1), (103, 'Four', 4)]).toDF('ID:int, name:string, parentID:int')\n",
        "tab1.join(tab2, tab1.ID == tab2.parentID).show()\n",
        "tab1.join(tab2, tab1.ID == tab2.parentID, 'left').show()\n",
        "tab1.join(tab2, tab1.ID == tab2.parentID, 'right').show()\n",
        "tab1.join(tab2, tab1.ID == tab2.parentID, 'full').show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FbKSw8mFJxw1"
      },
      "source": [
        "###Examples of aggregate functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Q-LPpNaJxw2",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "tab3 = sc.parallelize([(1, 10), (1, 20), (1, 30), (2, 40), (2,50)]).toDF('groupID:int, amount:int')\n",
        "tab3.groupby('groupID').max().show()\n",
        "tab3.groupby('groupID').sum().show()\n",
        "x = tab3.groupby('groupID')\n",
        "x.agg({'amount':'sum', 'amount':'max'}).show()\n",
        "from pyspark.sql import functions as F\n",
        "x.agg(F.sum('amount'), F.max('amount')).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f5J3KIQfJxw5"
      },
      "source": [
        "###Examples of reading a CSV directly into a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tJWVKyXeJxw6",
        "colab": {}
      },
      "source": [
        "filename = '/home/student/ROI/Spark/datasets/finance/CreditCard.csv'\n",
        "df4 = spark.read.load(filename, format = 'csv', sep = ',', inferSchema = True, header = True)\n",
        "df4.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GmIrAg1KJxxA",
        "colab": {}
      },
      "source": [
        "df4 = spark.read.format('csv').option('header','true').option('inferSchema','true').load(filename)\n",
        "df4.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JQVzZUyVJxxE",
        "colab": {}
      },
      "source": [
        "df4 = spark.read.csv(filename, header = True, inferSchema = True)\n",
        "df4.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TlpJkJKkJxxI",
        "colab": {}
      },
      "source": [
        "df4.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dDGFU2bwJxxN"
      },
      "source": [
        "###***LAB:*** Read the Products file from the JSON folder and categories from the CSVHeaders folder, then join them displaying just the product and category IDs and names, and sort by categoryID then productID. \n",
        "\n",
        "\n",
        "**Hint:** Drop the ambiguous column after the join."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTxpSZd7JxxP",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2pxxBU4AJxxS"
      },
      "source": [
        "###Change the name of the City column to CityCountry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0RUqQyieJxxU",
        "colab": {}
      },
      "source": [
        "cols = df4.columns\n",
        "cols[0] = 'CityCountry'\n",
        "df4 = df4.toDF(*cols)\n",
        "df4.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pk-WlDwNJxxW"
      },
      "source": [
        "###Apply a custom UDF to columns to separate the City and Country and convert the Date into a date datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4dcWXI1iJxxX",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import to_date\n",
        "import datetime\n",
        "\n",
        "def city(x):\n",
        "    return x[:x.find(',')]\n",
        "def country(x):\n",
        "    return x[x.find(',') + 1 :]\n",
        "\n",
        "df5 = df4.withColumn('City', udf(city, StringType())(df4.CityCountry)) \\\n",
        "      .withColumn('Country', udf(country, StringType())(df4.CityCountry)) \\\n",
        "      .withColumn('Date', to_date(df4.Date, 'dd-MMM-yy')) \\\n",
        "      .drop(df4.CityCountry)\n",
        "df5.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJShtBBYJxxe"
      },
      "source": [
        "###DataFrames can be written to a variety of file formats. Here we are writing it to JSON."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lds2CsoKJxxf",
        "colab": {}
      },
      "source": [
        "df5.write.json('/home/student/Spark/CreditCard.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h1hZdPH5Jxxm"
      },
      "source": [
        "###Read a JSON file into a DataFrame, but note that we lose the datatypes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jYzveIUNJxxn",
        "colab": {}
      },
      "source": [
        "df6 = spark.read.json('/home/student/Spark/CreditCard.json')\n",
        "df6.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ex6IIfYYJxxp"
      },
      "source": [
        "###Create a schema that can be used to import a file and directly name the columns and convert them to the desired data type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BEYmzqk8Jxxq",
        "colab": {}
      },
      "source": [
        "schema = StructType([\n",
        "    StructField('Date', DateType()), \n",
        "    StructField('Card Type', StringType()),\n",
        "    StructField('Exp Type', StringType()),\n",
        "    StructField('Gender', StringType()),\n",
        "    StructField('Amount', FloatType()),\n",
        "    StructField('City', StringType()),\n",
        "    StructField('Country', StringType())\n",
        "])\n",
        "df6 = spark.read.json('/home/student/Spark/CreditCard.json', schema = schema)\n",
        "df6.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}