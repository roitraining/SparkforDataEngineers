{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Ch04_ClusterAnalysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roitraining/SparkforDataEngineers/blob/Development/Ch04_ClusterAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6KkEjHKpxwC",
        "colab_type": "text"
      },
      "source": [
        "### Initialize the spark environment and load the helper functions we have provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jva8hK90pxwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/home/student/ROI/SparkProgram')\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib as mp\n",
        "import numpy\n",
        "from matplotlib import pyplot as plt\n",
        "#from IPython.display import display\n",
        "\n",
        "import pyspark_helpers as pyh\n",
        "sc, spark, conf = pyh.initspark()\n",
        "from pyspark_helpers import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbzStKdFpxwM",
        "colab_type": "text"
      },
      "source": [
        "### Read in a simple dataset of latitudes and longitudes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM_IDpZzpxwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'superchargers.csv'\n",
        "df = spark.read.csv(f'/home/student/ROI/Spark/datasets/finance/{filename}', header = True, inferSchema = True)\n",
        "display(df)\n",
        "\n",
        "# Save a pointer to the raw data\n",
        "dfRawFile = df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d12RxW8pxwR",
        "colab_type": "text"
      },
      "source": [
        "### Visualize this dataset using pandas. Normally you don't do this in spark but it is helpful here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf3-joDrpxwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "p = df.toPandas()\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(p.loc[:,'lat'],p.loc[:,'lng'],'o')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNtS0k3bpxwV",
        "colab_type": "text"
      },
      "source": [
        "### Turn the features into a big vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juAe-hNYpxwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "vecAssembler = VectorAssembler(inputCols=[\"lat\", \"lng\"], outputCol=\"features\")\n",
        "dfML = vecAssembler.transform(df)\n",
        "display(dfML)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KDNJ3GMpxwb",
        "colab_type": "text"
      },
      "source": [
        "### Load the KMeans class and train the model. Evaluate how good it performs for several different cluster counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqbL9hrJpxwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "def evaluateCluster(model, df):\n",
        "    wssse = model.computeCost(dfML.select('features'))\n",
        "    print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
        "\n",
        "    evaluator = ClusteringEvaluator()\n",
        "\n",
        "    predictions = model.transform(df)\n",
        "    silhouette = evaluator.evaluate(predictions)\n",
        "    print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "\n",
        "    # Shows the result.\n",
        "    centers = model.clusterCenters()\n",
        "    print(\"Cluster Centers: \")\n",
        "    for center in centers:\n",
        "        print(center)\n",
        "\n",
        "for k in range(2, 5):\n",
        "    print ('Number of clusters', k)\n",
        "    kmeans = KMeans().setK(k).setSeed(1)\n",
        "    model = kmeans.fit(dfML.select('features'))\n",
        "    evaluateCluster(model, dfML.select('features'))\n",
        "    print()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiQS7aWcpxwf",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the cluster results graphically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7PxeQFNpxwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "CLUSTERS = 2\n",
        "kmeans = KMeans().setK(CLUSTERS).setSeed(1)\n",
        "model = kmeans.fit(dfML.select('features'))\n",
        "predictions = model.transform(dfML)\n",
        "centroids = model.clusterCenters()\n",
        "\n",
        "for i in range(CLUSTERS):    \n",
        "    p = predictions.select('lat', 'lng').where(f'prediction = {i}').toPandas()\n",
        "    plt.plot(p.loc[:,'lat'],p.loc[:,'lng'],'o')\n",
        "    plt.plot(centroids[i][0], \n",
        "           centroids[i][1],'kx')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhLUXEbspxwk",
        "colab_type": "text"
      },
      "source": [
        "### Use and elbow chart to help visualize what is the optimal number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr1gAeuKpxwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "def plot_elbow(df, cluster_cnt = 6):\n",
        "    import numpy as np\n",
        "    CLUSTERS = range(2, cluster_cnt)\n",
        "    scores = [KMeans().setK(c).setSeed(1).fit(df).computeCost(df)\n",
        "              for c in CLUSTERS]\n",
        "    print(scores)\n",
        "    plt.plot(CLUSTERS, scores)\n",
        "    plt.xlabel('Number of Clusters')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Elbow Curve')\n",
        "    plt.xticks(np.arange(2, cluster_cnt))\n",
        "\n",
        "plot_elbow(dfML.select('features'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQu0GCM6pxwq",
        "colab_type": "text"
      },
      "source": [
        "### Work in progress below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw65R2tZpxwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "lda = LDA(k=10, maxIter=10)\n",
        "model = lda.fit(dfML.select('features'))\n",
        "\n",
        "ll = model.logLikelihood(dfML)\n",
        "lp = model.logPerplexity(dfML)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))\n",
        "\n",
        "# Describe topics.\n",
        "topics = model.describeTopics(3)\n",
        "print(\"The topics described by their top-weighted terms:\")\n",
        "topics.show(truncate=False)\n",
        "\n",
        "# Shows the result\n",
        "transformed = model.transform(dataset.select('features'))\n",
        "transformed.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHf9aHUlpxwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.clustering import GaussianMixture\n",
        "\n",
        "# loads data\n",
        "gmm = GaussianMixture().setK(2).setSeed(1)\n",
        "model = gmm.fit(dfML.select('features'))\n",
        "\n",
        "print(\"Gaussians shown as a DataFrame: \")\n",
        "model.gaussiansDF.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzrQlYHYpxwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}