{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/roitraining/SparkforDataEngineers/blob/Development/Ch02_DataFrames_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_H1eukfMZpL"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "rootpath = '/home/student/ROI/Spark/'\n",
    "datapath = f'{rootpath}datasets/'\n",
    "sys.path.append(rootpath)\n",
    "from pyspark_helpers import *\n",
    "sc, spark, conf = initspark()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArgIK-HvMZpS"
   },
   "source": [
    "### ***LAB:*** Use the regions and territories RDDs from the previous lab and convert them into DataFrames with meaningful schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeFTvJLeMZpT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regions = sc.textFile('hdfs://localhost:9000/regions')\n",
    "regions = regions.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1]))\n",
    "regionsdf = regions.toDF('RegionID:int, RegionName:string')\n",
    "regionsdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GToUtN0MZpY"
   },
   "outputs": [],
   "source": [
    "territories = sc.textFile('hdfs://localhost:9000/territories')\n",
    "territories = territories.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[2])))\n",
    "territoriesdf = territories.toDF('TerritoryID:int, TerritoryName:string, RegionID: int')\n",
    "territoriesdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HPkB75FJMZpc"
   },
   "source": [
    "### ***LAB:*** Using the df3 DataFrame, answer the following questions:\n",
    "\n",
    "*   How many Platinum card purchases were there with a discount above $100?\n",
    "*   Find the ten biggest discount amounts earned by women and show just the purchase amount, discount, and date.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1B9apKoTMZpd"
   },
   "outputs": [],
   "source": [
    "filename = f'{datapath}/finance/CreditCard.csv'\n",
    "cc = sc.textFile(filename)\n",
    "first = cc.first()\n",
    "cc = cc.filter(lambda x : x != first)\n",
    "import datetime\n",
    "cc = cc.map(lambda x : x.split(',')) \n",
    "cc = cc.map(lambda x : (x[0][1:], x[1][1:-1], datetime.datetime.strptime(x[2], '%d-%b-%y').date(), x[3], x[4], x[5], float(x[6])))\n",
    "df = spark.createDataFrame(cc)\n",
    "df = cc.toDF('City: string, Country: string, Date: date, CardType: string, TranType: string, Gender: string, Amount: double')\n",
    "df2 = df.withColumn('Discount', df.Amount * .03)\n",
    "df3 = df2.drop(df2.Country)\n",
    "\n",
    "\n",
    "print(df3.where(\"CardType = 'Platinum' and Discount > 100\").count())\n",
    "display(df3.where(\"Gender = 'F'\").orderBy('Amount', ascending = False).select('Amount', 'Discount', 'Date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzJj7KPoMZpj"
   },
   "source": [
    "### ***LAB:*** Read the Products file from the JSON folder and categories from the CSVHeaders folder, then join them displaying just the product and category IDs and names, and sort by categoryID then productID. \n",
    "\n",
    "**Hint:** Drop the ambiguous column after the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MRbCDoUMZpl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products = spark.read.json(f'{datapath}/northwind/JSON/products')\n",
    "#products.show()\n",
    "products.printSchema()\n",
    "\n",
    "categories = spark.read.csv(f'{datapath}/northwind/CSVHeaders/categories', header = True, inferSchema = True)\n",
    "#categories.show()\n",
    "categories.printSchema()\n",
    "\n",
    "c = categories\n",
    "p = products\n",
    "c.join(p, c.CategoryID == p.categoryid).drop(p.categoryid).select('CategoryID', 'CategoryName', 'productid', 'productname').orderBy('categoryid', 'productid').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJx2rbFyMZpo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Day2_Labs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
