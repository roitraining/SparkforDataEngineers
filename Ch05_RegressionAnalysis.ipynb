{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the spark environment and load the helper functions we have provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/student/ROI/SparkProgram')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "#from IPython.display import display\n",
    "\n",
    "import pyspark_helpers as pyh\n",
    "sc, spark, conf = pyh.initspark()\n",
    "from pyspark_helpers import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in a simple dataset of Boston Housing Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'avocado.csv'\n",
    "#filename = 'HousingData.csv'\n",
    "filename = 'boston.csv'\n",
    "df = spark.read.csv(f'/home/student/ROI/Spark/datasets/finance/{filename}', header = True, inferSchema = True)\n",
    "display(df)\n",
    "df.printSchema()\n",
    "\n",
    "# Save a pointer to the raw data\n",
    "dfRaw = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "col = 'TOWN'\n",
    "indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "x1 = indexer.fit(df).transform(df).select(col, col+'_Index').distinct()\n",
    "display(x1.orderBy(col))\n",
    "display(x1.orderBy(col+'_Index'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = pyh.StringIndexEncode(df, ['TOWN', 'TRACT'])\n",
    "display(x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'TOWN'\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "display(encoder.fit(x2).transform(x).orderBy(col + '_Index'))\n",
    "\n",
    "x = pyh.OneHotEncode(x2, ['TOWN', 'TRACT'])\n",
    "display (x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#sns.distplot(df.toPandas()['MEDV'])\n",
    "\n",
    "sns.distplot(df.where('MEDV < 48').toPandas()['MEDV'])\n",
    "print(df.columns)\n",
    "\n",
    "# If we want to filter out the outliers\n",
    "dfRaw = dfRaw.where('MEDV < 48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filename == 'avocado.csv':\n",
    "    df = dfRaw.withColumnRenamed('4046','PLU4046').withColumnRenamed('4225','PLU4225').withColumnRenamed('4770','PLU4770')\n",
    "    df.createOrReplaceTempView('dfRaw')\n",
    "    df.printSchema()\n",
    "\n",
    "    sql = '''select AveragePrice as target, `Total Volume` as totalvolume\n",
    "    , PLU4046, PLU4225, PLU4770\n",
    "    , `Small Bags` as smallbags, `Large Bags` as largebags, `XLarge Bags` as xlargebags\n",
    "    , type, year, region\n",
    "    FROM dfRaw'''\n",
    "\n",
    "    df = spark.sql(sql)\n",
    "    print(df)\n",
    "\n",
    "    numeric_features = ['totalvolume','PLU4046', 'PLU4225', 'PLU4770', 'smallbags', 'largebags', 'xlargebags']\n",
    "    categorical_features = ['type', 'year','region']\n",
    "    target_label = 'target'\n",
    "    print(df.take(1))\n",
    "else:\n",
    "    numeric_features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO']\n",
    "    categorical_features = [] #['TOWN', 'TRACT']\n",
    "    target_label = 'MEDV'\n",
    "    df = dfRaw.select(categorical_features + numeric_features + [target_label])\n",
    "    df.printSchema()\n",
    "\n",
    "print ('******')\n",
    "display(df.describe())\n",
    "\n",
    "print ('******')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn the dataframe into vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imp\n",
    "# imp.reload(pyh)\n",
    "\n",
    "# df10 = pyh.StringIndexEncode(df, categorical_features)\n",
    "# display(df10)\n",
    "# df11 = pyh.OneHotEncode(df10, categorical_features)\n",
    "# display(df11)\n",
    "# df12 = pyh.AssembleFeatures(df11, categorical_features, numeric_features, 'target', False)\n",
    "# display(df12)\n",
    "\n",
    "dfML = pyh.MakeMLDataFrame(df, categorical_features, numeric_features, target_label, False)\n",
    "display(dfML)\n",
    "dfML.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dfML.randomSplit([.7,.3], seed = 1000)\n",
    "print (f'Training set row count {train.count()}')\n",
    "print (f'Testing set row count {test.count()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='target', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(train)\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "print(\"Root Mean Squared Error: {}\\nR Squared (R2) {}\".format(lrModel.summary.rootMeanSquaredError, lrModel.summary.r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictions = lrModel.transform(test)\n",
    "display(lrPredictions.select(\"prediction\",\"target\",\"features\"), 30)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lrEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\",metricName=\"r2\")\n",
    "testResult = lrModel.evaluate(test)\n",
    "print(\"Root Mean Squared Error on Test set: {}\".format(testResult.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'target')\n",
    "dtModel = dt.fit(train)\n",
    "dtPredictions = dtModel.transform(test)\n",
    "important = dtModel.featureImportances\n",
    "print(type(important), important)\n",
    "#importantDict = zip(important[1], important[2])\n",
    "#print (importantDict)\n",
    "print (important[3])\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "dtEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\",metricName=\"rmse\")\n",
    "testResult = dtEvaluator.evaluate(dtPredictions)\n",
    "print(\"Root Mean Squared Error: {}\".format(testResult))\n",
    "dfML.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'target', maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "gbtPredictions = gbtModel.transform(test)\n",
    "display(gbtPredictions.select('prediction', 'target', 'features'), 20)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "gbtEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\",metricName=\"rmse\")\n",
    "testResult = gbtEvaluator.evaluate(gbtPredictions)\n",
    "print(\"Root Mean Squared Error: {}\".format(testResult))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
